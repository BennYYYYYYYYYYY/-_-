'''
1. RNN、LSTM 的問題：
    傳統的序列模型中，每個輸入通常會生成一個隱藏狀態(hidden state)，然後這個隱狀態會傳遞到下一個時間步驟，依次處理整個序列。
    這類模型可以在一定程度上記住前面的上下文信息，但是當序列變長時，它們很難有效地記住早期的信息，尤其是那些和當前輸入有強依賴關係的早期部分。

    例如："I live in Taiwan and I love the food here." 想要讓模型理解 "here" 指的是 "Paris" 這個詞很難(因為兩者之間隔了很多詞)。

     
2. 注意力機制：
    注意力機制在 2014 年首次被引入來幫助解決這個長距離依賴問題(在機器翻譯的情況下)。
    核心是：當處理序列中的某個詞時，不需要依賴於固定大小的隱狀態向量，而是可以動態地「關注」序列中其他對這個詞有貢獻的詞，並分配不同的權重來表示它們的重要性。

    注意力權重：
        給定一個序列中的詞彙，當想要計算這個詞與序列中其他詞的「相關性」。
        這種相關性通常是通過 word embeddubg 之間的相似性來計算的。常用的方法是使用點積來度量詞之間的相似性(矩陣乘法，算相似程度)。

        1. 輸入序列表示：假設有一個詞序列，每個詞都被轉換為一個向量，這些向量可以表示為 x1, x2, x3, x4, x5.... ,xn

        2. 注意力分數計算：對於當前詞(假設是xi)，計算它與序列中其他詞(假設是xj)的相關性分數(注意力分數)。
            分數可以通過計算 xi 與 xj 的點積來獲得 => score(xi, xj) = xi @ xj

        3. 注意力權重：為了將這些分數轉換為權重(機率分布)，對這些分數應用 softmax 函數，將它們標準化到0到1之間，使得權重的總和為1。這樣可以得到每個詞對當前詞的相對重要性。

'''