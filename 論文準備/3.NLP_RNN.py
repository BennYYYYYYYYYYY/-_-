'''
自然語言的推斷(Inference)，除了要考慮上下文的關係以外，還要考慮「記憶力」。
例如：
治水 -> 大禹；齊天大聖 -> 孫悟空

因此，NLP演算法要提供準確率，模型就必須額外添加「上下文關係」+「記憶力」功能。

介紹順序： RNN ->  LSTM -> Attention Mechanism-> Transformer -> BERT
'''

'''
1. 循環神經網路(Recurrent Neural Network ,RNN)


    1. 傳統神經網絡，不論是全連接神經網絡(Fully Connected Neural Network)、卷積神經網絡(Convolutional Neural Network)，都有以下缺點：
        1. 只能處理固定大小的輸入，無法處理序列數據(這些數據具有時間依賴性，即當前的數據會受到歷史數據的影響，而傳統神經網絡無法捕捉到這種關聯) 
        2. 無法處理輸入數據之間的依賴關係。每個輸入之間是獨立的，沒有「記憶」或「上下文」信息。
 
        
    2. RNN 概念
        RNN 輸出的計算過程中會考慮過去的輸入狀態，RNN 中的每個節點不僅接收當前輸入，還會接收上一個時間步驟的隱藏狀態(記憶)。
        RNN 之所以叫「循環」神經網絡，是因為它在序列中不斷重複使用相同的網路單元。

        
    3. RNN 結構
        RNN 具有一個「隱藏層(Hidden State)」，這個隱藏層負責保留序列數據中的歷史信息。
        在每個時間步，RNN 都會接收一個輸入，然後基於當前輸入和前一個時間步的隱藏狀態來更新新的隱藏狀態，並產生當前的輸出。

            1. 輸入：假設序列輸入為 X = (x1, x2, x3, x4, ... ,xt)，每個 xt 是在 t 時間點的輸入
            2. 隱藏狀態(歷史訊息)：在每個時間步 t 都會有一個隱藏狀態 ht，ht 保存了「之前的狀態訊息」
            3. 輸出：RNN 在每個時間步產生一個輸出 yt，這個輸出與當前的隱藏狀態相關。   

            ht = W * h(t-1) + U * xt + b                              y = V * ht
            隱藏狀態(t時) = 權重*隱藏狀態(t-1) + 權重*輸入(t) + bias   輸出 = 權重 * (權重*隱藏狀態(t-1) + 權重*輸入(t) + bias)

            
    4. 嵌入層(Embedding Layer)
        文本處理時，需要將詞轉換為數字，常見的數字化方式有兩種。

        1. One-Hot Encoding：每個詞被表示為一個高維稀疏向量，向量的維度等於詞彙表大小。難以捕捉詞語之間的語義關係。
        2. Word Embedding：每個詞被表示為一個低維的稠密向量，這些向量能夠捕捉詞語之間的語義關聯(嵌入層要做的事)

        例如：["I", "love", "natural", "language", "processing"]

        1. One-Hot Encoding：

            [
            [1, 0, 0, 0, 0],  # "I"
            [0, 1, 0, 0, 0],  # "love"
            [0, 0, 1, 0, 0],  # "natural"
            [0, 0, 0, 1, 0],  # "language"
            [0, 0, 0, 0, 1]   # "processing"
            ]

        2. Word Embedding：

            [        
            [ 0.12, -0.25,  0.30],  # "I"
            [ 0.10,  0.25, -0.15],  # "love"
            [ 0.35,  0.22,  0.18],  # "natural"
            [-0.18,  0.27,  0.55],  # "language"
            [ 0.48, -0.40,  0.31]   # "processing"
            ]

            
    5. 長期依賴問題
        長期依賴是指在處理序列數據時，模型需要記住很久之前的信息，來影響當前的預測或決策
        例如："The dog that I saw yesterday was running fast"，處理到"running"時，會需要「主語」這一信息，但 "dog"在很久之前就出現了。
        當序列過長時，RNN 很難記住早期出現的信息。這就是「長期依賴問題」。


    6.  梯度共享、梯度消失、梯度爆炸
        RNN 的訓練過程叫做時間上的反向傳播(Backpropagation Through Time, BPTT)     

        1.  梯度共享
            RNN 使用相同的權重矩陣在每個時間步更新隱藏狀態，在反向傳播過程中，每個時間步的梯度也是共享的。
            所以當梯度開始消失或爆炸時，這種影響會在整個序列中積累，特別是在長序列上。


        2. 梯度消失(Vanishing Gradient problem)
            梯度消失是指當反向傳播的梯度在傳遞過程中變得越來越小，最終接近於零，這會導致網路的權重更新非常緩慢，甚至無法更新。
            在 RNN 的運算中，每個時間在用上一個時間隱藏狀態(ht-1)來更新當前的隱藏狀態(ht)，這涉及到激活函數(例如Sigmoid函數導數在0~1)，它們的導數通常小於 1。當梯度隨著時間步不斷地進行反向傳播時，這些小於 1 的導數會被不斷相乘，導致梯度變得越來越小。
            例如：序列長度(t) = 4, 使用Sigmoid激活函數, 權重矩陣W = 0.5
                  
                  隱藏狀態(t) = 激活函數(權重 * 隱藏狀態(t-1))
                  h1 = Sigmoid (0.5 * h0)
                  h2 = Sigmoid (0.5 * h1)
                  h3 = Sigmoid (0.5 * h2)
                  h4 = Sigmoid (0.5 * h3)
         
            1. 從 h4 反向傳播：∂L/∂h4 = 算得出來(假設為1)
                因為已知輸出y4(模型的最終預測結果)，所以可以計算出L(損失函數用來衡量預測結果和真實值的差距)
                且已知h4計算而來，因此，可以對L的h4微分，得知h4對L的影響程度
            
            2. 對損失函數L的h3進行偏微分(h3對L的影響程度) = (對L的h4偏微分結果) * 權重矩陣 * 激活函數對h3的導數(Sigmoid'(h3)) 
                => ∂L/∂h3 = 1(已算出的∂L/h4) * 0.5 * 0.25(假設sigmoid的導數) = 0.125    
            
            3. ∂L/∂h2 => 0.125(∂L/∂h3) * 0.5 * 0.25 = 0.015625
            .
            .
            .
             超小的梯度幾乎無法更新網路的權重，導致 RNN 無法記住早期的輸入(特別是當序列很長時)，早期的關鍵信息會被「忘記」

            
        3. 梯度爆炸(Exploding Gradient problem)
            梯度爆炸是指在反向傳播過程中，梯度值變得非常大，導致權重更新幅度過大，使模型的學習過程變得不穩定，甚至崩潰。
            如果在反向傳播過程中，網路中的權重矩陣或激活函數的導數值大於 1，那麼每次反向傳播時，梯度就會被不斷放大。當時間步數很多時，這些放大的梯度會變得極其巨大，導致梯度爆炸。
            例如：
            ∂L/∂h3 = 1(已算出的∂L/h4) * 1.5 * 1(ReLU的導數) = 1.5
            ∂L/∂h2 = 1.5(∂L/h3) * 1.5 * 1(ReLU的導數) = 2.25
            ∂L/∂h1 = 2.25(∂L/h2) * 1.5 * 1(ReLU的導數) = 3.375
            過大的梯度會使網路的權重更新變得不穩定，使網路崩潰。

'''


