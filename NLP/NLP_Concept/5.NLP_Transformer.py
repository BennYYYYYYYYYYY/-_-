'''
1. Transformer 
    與傳統的 RNN 或 LSTM 模型不同，Transformer 並不依賴於序列數據的順序處理。
    而是基於注意力機制，尤其是自注意力機制(Self-Attention)，來並行處理整個序列。

    Transformer 有兩個主要組件：
        
        1. 編碼器(Encoder)：負責理解輸入序列。
        2. 解碼器(Decoder)：生成輸出序列。

    
2. tranformer 架構
    transformer 由多個堆疊的 encoder 層和 decoder 層組成。

    1. encoder：每一層包含兩個主要模塊

        1. 自注意力層 (Self-Attention Layer)
            自注意力機制是 Transformer 的核心，它是如何實現從序列中提取上下文信息的關鍵。
            概念是，序列中的每個詞能夠「關注」句子中的其他詞，並根據其相關性調整自己對這些詞的權重。具體有四步驟。

                1. 對每個詞進行 Query(查詢)、Key(鍵)、Value(值)的變換：
                    每個詞最初是表示為 word2vec 這層會將這些向量通過學習的權重矩陣轉換為三個向量(Q、K、V)
                        
                        1. Query 向量表示當前詞希望「查詢」句子中其他詞的相關性。
                        2. Key 向量表示句子中其他詞的特徵，用來跟 Query 詞進行匹配。
                        3. Value 向量代表具體的語義，有了注意力權重(從QK相似度計算而來) 後，將根據這些權重來加權計算結果。

                2. 計算相似性分數：
                    對於每個 Query 和句子中每個 Key，計算它們之間的點積(相似性)，表示兩個詞之間的相似程度。

                3. Softmax：通過 Softmax 將這些相似度分數轉換為注意力權重，這些權重可以解釋為 Query 詞對句子中每個詞的「關注程度」(越相似、越有關聯，關注度/機率越高)。

                4. 加權和：將每個 Value 向量根據這些權重進行加權求和，這樣我們就可以得到一個新的向量，這個向量表示當前詞在考慮句子中其他所有詞的上下文信息之後的表達。

                自注意力機制的特點在於：每個詞的最終表示並不僅僅依賴於它本身，而是根據它和句子中其他詞的相關性進行加權和，因此它能夠捕捉句子中的長距離依賴。

                
            多頭注意力機制(Multi-Head Attention)：
                多頭注意力是 self-attention 的擴展，將這個QKV過程複製多次，並在不同的頭(head)中並行計算多組注意力。
                -> 將 Query、Key、Value 分別分成多組(頭)，每組都計算一組注意力分數，最終將多組結果拼接起來並通過線性變換進行融合。

                MultiHead(Q,K,V) = Concat(head1, head2, head3.....,headn) * Wo(線性轉換矩陣)

            
            前饋神經網路(Feedforward Neural Network)：
                在每一層的自注意力機制之後，還有一個前饋神經網路，作用是進一步非線性地轉換每個詞的向量。包含兩層全連接層：
                    1. 第一層將 word embedding 的維度擴展到更高的維度。
                    2. 第二層將其映射回原來的維度 (能夠幫助模型學習更加複雜的語義變換)

                    
        2. 位置編碼 (Positional Encoding)：
            Transformer 不像 RNN 或 LSTM 一樣按照順序處理序列。所以 Transformer 並不能理解序列的順序信息。
            為了解決這個問題，Transformer 引入了 Positional Encoding，它為每個詞向量增加了位置信息，讓模型能夠知道詞語在序列中的相對位置。
    
'''