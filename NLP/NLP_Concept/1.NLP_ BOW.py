'''
NLP：文字 -> 向量 -> 分析/深度學習建模


1. 詞袋(Bag of Words, BOW)
詞袋模型將文本表示為「詞的集合」，忽略詞的順序與語法結構，只關注詞的出現頻率。在詞袋模型中，每個詞的順序並不重要，而是將文本視作一個詞的「袋子」。 

    1. BOW 作法：分詞 -> 前處理 -> 去除停用字 -> 字詞出現次數統計
        
        1. 分詞(Tokeniztion)：
            將文章中的每個詞彙切開，整理成文字表。
            英文較簡單(以空格或句點切開)，中文較複雜(需用特殊方式處理)

        2. 前置處理(Preprocessing)：
            詞形還原、轉換小寫等等。

        3. 去除停用詞(Stop Words)：
            be動詞、助動詞、代名詞、竊系詞等等，不具特殊意義的詞彙稱為「停用詞」。
            需要將他們刪除，不然統計結果中，出現最多次的都會是停用詞。

        4. 詞彙出現次數統計：
            計算詞彙在文章中出現的次數，並由高排到低。


            
2. TF-IDF (Term Frequency - inverse Document Frequency) 
BOW的缺點是，有些詞彙雖然不是停用詞，也經常出現，但與文章無相關性，對猜測全文大意沒有幫助。
所以學者提出TF-IDF演算法，他會針對跨文件長出現的詞彙給較低的分數，例如，如果 only 在每個文章都有出現，TF-IDF的分數就較低。

    1. 字詞頻率 (Term Frequency): 
        計算每文章字詞頻率
    
    2. 逆文件頻率(Inverse Document Frequency): 一個單字出現在文章數目的逆向次數。
      如果該字太常出現，就顯得不重要，例如:「你」、「我」、「他」、「了」、「吧」....這種不具有指標性的主詞或語氣詞。他的加權數值就會顯低上許多。
      會取log的原因在於隨著每個字詞的增加，如【10 - 9】與【1000 - 999】之差異。一個是1/10、一個卻是1/1000的差距。通過取對數來壓縮這種比例，使得頻繁出現的詞其權重下降速度變得平緩，不會極端地降低其他詞的權重。
       
       TF-IDF = tf * idf 
              = tf(x,y) * log(N/dfx) 
              = 詞 x 在文件 y 中出現的次數(相對於該文件中的所有詞) * [log(文章總數量/包含詞語x的文件數目)] 

        舉例：
            doc_1: "I love eating an apple every day"
            doc_2: "She bought an apple and an orange"
            doc_3: "Apple pie is delicious"
            doc_4: "He prefers bananas to apples"

        計算 "apple" tf：
            doc_1: 6 個字中出現 apple 1次 = 1/6
            doc_2: 7 個字中出現 apple 1次 = 1/7
            doc_3: 4 個字中出現 apple 1次 = 1/4
            doc_4: 5 個字中出現 apple 1次 = 0/5

        計算 "apple" idf：
            文章總數 = 4
            df(包含"apple"的文章數量) = 3
            idf = log(4/3) = 0.1249

        計算 tf-idf：
            doc_1: 1/6 * 0.1249
            doc_2: 1/7 * 0.1249
            doc_3: 1/4 * 0.1249
            doc_4: 0/5 * 0.1249
        
    計算TF-IDF特徵值, 輸出會是一個稀疏矩陣，表示每個文件中的每個特徵詞的 TF-IDF 分數(final_traindata_tfdif)
      | Document    | and | bird   | cat    | dog    | 
      |-------------|-----|--------|--------|--------|
      | cat and dog | 0   | 0      | 0.176  | 0.176  |
      | dog and bird| 0   | 0.176  | 0      | 0.176  |
      | cat and bird| 0   | 0.176  | 0.176  | 0      |

            
3. 詞形還原 (stemm 和 lemmatization)
目的是將單詞還原為其詞根形式，以便統一處理詞語的不同變形。詞形還原有兩種主要的方法：stemming 和 lemmatization

    1. Stemming (依字根做詞形還原)
        Stemming 是一種將詞彙削減為詞幹的技術，不管字的涵義，直接進行字根詞形還原。
        例如：
        keeps -> 減去s -> keep
        catching -> 減去ing -> catch
        his -> 減去s -> hi (發生錯誤)

    2. Lemmatization (依字典規則做詞形還原)
        Lemmatization 會查詢字典，依單字的不同進行詞形還原
        例如：
        running -> run
        better -> good

        
4. 詞向量(Word2Vec)： 

    1. BOW的問題為
        1. 無法表現詞之間的相似性，例如"language" 和"processing" 之間的語義關聯
        2. 如果詞袋很大，則每個句子的向量維度就會非常高，導致效率低下且浪費內存。
        3. 稀疏矩陣很多位置都是零，因為一個句子只會使用很少的一部分詞彙表中的詞。

        為了解決這些問題，提出了「詞向量」的概念。詞向量的目的是把每個詞轉換為一個實數向量(數字列表)，這些向量能夠捕捉詞語之間的語義和關聯。
        例如：
        "apple" 可能被表示為 [0.21, -0.30, 0.52, ...]。
        "fruit" 可能被表示為 [0.20, -0.31, 0.50, ...]。
        這些向量表示的詞之間，語義相近的詞其向量也會很接近。可以通過計算這些數字的距離來判斷詞語之間的相似性。

        
    2. 詞嵌入(Word Embedding)
        詞嵌入是一種將詞語轉換為向量的技術。
        可以把詞嵌入理解為詞語的數字化「壓縮版」，這些數字同時保留了詞語的語義關聯。詞嵌入生成的詞向量是稠密的實數向量，每個詞語被表示為一個具有多個數字的列表。

        假設用 3 維空間來表示詞語 (實際上常用 100 到 300 維)
            "apple" 可能被表示為 [0.21, -0.30, 0.52]。
            "fruit" 可能被表示為 [0.20, -0.31, 0.50]。

        1維可能表示「食物與否」，如果是食物，值接近1。
        2維可能表示「固體與否」，固體食物可能接近0.5，液體食物接近0。
        3維可能表示「甜味」，值越大表示越甜。

        相比於詞袋模型產生的高維、稀疏的向量，詞嵌入產生的向量是低維且稠密的，稠密向量中的每個維度都有意義，不同的維度可能代表不同的語義特徵，並且數字不會是零，這樣能提高計算效率。

        
    3. 生成詞向量
        Word2Vec 是由 Google 在 2013 年提出的一種 Word Embedding 技術。它的基本想法是基於大量的文本學習詞語之間的語義關係，從而生成每個詞的向量表示。
        Word2Vec 有兩種主要的架構: CBOW(連續詞袋模型) 和 Skip-gram。
        
            1. CBOW(Continuous Bag of Words)
                CBOW 的工作原理是：根據上下文(周圍的詞)來預測中心的詞。
                例如："I love … language processing"，來預測被省略的詞 ("natural")。

            2. Skip-gram
                與 CBOW 相反，Skip-gram 模型是根據中心詞來預測上下文詞。
                例如：給定一個詞"natural"，並嘗試預測這個詞周圍的詞"love", "language", "processing"


    4. 負樣本抽樣 (Negative Sampling)
        Skip-gram 模型中，有一個重要的技術叫負樣本抽樣，這是為了加快模型訓練速度而設計。

        1. 為何需要負樣本抽樣
            Skip-gram 模型是通過中心詞來預測上下文詞，但詞彙表可能非常大，不可能每次都對所有詞進行計算，這樣會非常慢。

        2.  負樣本抽樣工作原理
            為了提高效率，只會選擇一部分詞進行預測。除了正樣本以外(實際出現在上下文中的詞)，還會隨機選擇一些詞作為「負樣本」。
            負樣本是那些不應該出現在上下文中的詞，模型會學習把這些詞與中心詞區分開來。

            例如：中心詞是 "apple"，它的上下文詞應該是 "fruit"、"eat" 等。
            負樣本抽樣會隨機選擇一些不相關的詞(如"car"、"computer")作為負樣本，告訴模型這些詞不應該出現在 "apple" 的上下文中。


    5. n-gram：詞序關係
        n-gram 是一種將句子以不同長度切分為各個字詞的方法，n 即代表每次的切分長度。
        例如 「我喜歡你」 在 n = 1 的情形下會切分為 「我」 ，「喜」，「歡」，「你」
        假設 n = 2 ，即會切分為「我喜」，「喜歡」，「歡你」，以此類推。
        除此之外，一般在進行資料分析時，遇到如「我喜歡你」 此種因順序不同而具不同意義之句子時往往無法判讀其真實語意，而運用 n-gram 即可透過 「我喜」，「喜歡」 等不同欄位來釐清原句語意。
        往後若遇到原始資料中有四字成語、三字聯詞或兩字片語的情形，也可以分別利用 4-gram 、3-gram、2-gram 將這些詞彙切分出來

'''






